{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing NaiveRAG and ContextualRAG using RAGAS Metrics\n",
    "\n",
    "# This notebook evaluates and compares the performance of a standard 'naive' RAG system against our ContextualRAG implementation using RAGAS metrics.\n",
    "\n",
    "# RAGAS provides metrics for evaluating retrieval-augmented generation systems:\n",
    "# - Retrieval metrics (context precision, recall, relevancy)\n",
    "# - Generation metrics (faithfulness, answer relevancy)\n",
    "# - Overall system metrics (context utility)\n",
    "\n",
    "# Install required packages\n",
    "#!pip install ragas transformers langchain faiss-cpu datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92368812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting reportlab\n",
      "  Downloading reportlab-4.4.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/oleg/miniconda3/envs/llm/lib/python3.10/site-packages (from reportlab) (10.0.0)\n",
      "Requirement already satisfied: chardet in /home/oleg/miniconda3/envs/llm/lib/python3.10/site-packages (from reportlab) (5.2.0)\n",
      "Downloading reportlab-4.4.1-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: reportlab\n",
      "Successfully installed reportlab-4.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19baa93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## Set Up Test Data\n",
    "\n",
    "# We'll create a test dataset by using PDF files and generating test questions.\n",
    "\n",
    "# Set the path to your PDF documents\n",
    "PDF_FOLDER = \"./data/pdfs\"  # Update this to your PDF folder\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
    "\n",
    "# If you don't have sample PDFs, you can download some (e.g., research papers) or create them\n",
    "# Here's a function to create a sample PDF using ReportLab if needed\n",
    "def create_sample_pdf(output_path, num_pages=10):\n",
    "    from reportlab.lib.pagesizes import letter\n",
    "    from reportlab.pdfgen import canvas\n",
    "    import random\n",
    "    \n",
    "    # Sample content domains\n",
    "    domains = [\n",
    "        \"Machine Learning\", \n",
    "        \"Artificial Intelligence\", \n",
    "        \"Natural Language Processing\",\n",
    "        \"Computer Vision\",\n",
    "        \"Robotics\"\n",
    "    ]\n",
    "    \n",
    "    # Sample content for different domains\n",
    "    content = {\n",
    "        \"Machine Learning\": [\n",
    "            \"Machine learning is a branch of artificial intelligence that involves the development of algorithms.\",\n",
    "            \"Supervised learning requires labeled training data to learn from.\",\n",
    "            \"Unsupervised learning finds patterns in data without explicit labels.\",\n",
    "            \"Reinforcement learning involves an agent learning to make decisions by taking actions.\",\n",
    "            \"Feature engineering is the process of selecting and transforming variables for a machine learning model.\"\n",
    "        ],\n",
    "        \"Artificial Intelligence\": [\n",
    "            \"Artificial intelligence is the simulation of human intelligence processes by machines.\",\n",
    "            \"Strong AI would have the ability to apply intelligence to any problem, rather than just specific ones.\",\n",
    "            \"Expert systems were among the first truly successful forms of AI software.\",\n",
    "            \"The Turing Test was developed by Alan Turing to test a machine's ability to exhibit intelligent behavior.\",\n",
    "            \"AI ethics concerns the moral behaviors of humans as they design and implement artificial intelligence.\"\n",
    "        ],\n",
    "        \"Natural Language Processing\": [\n",
    "            \"Natural Language Processing (NLP) is a field of AI that focuses on interactions between computers and human language.\",\n",
    "            \"Sentiment analysis uses NLP to identify and categorize opinions in text.\",\n",
    "            \"Named Entity Recognition is the process of identifying and classifying key elements in text into predefined categories.\",\n",
    "            \"Language models like GPT and BERT have revolutionized NLP tasks.\",\n",
    "            \"Tokenization is the process of breaking down text into smaller chunks or tokens.\"\n",
    "        ],\n",
    "        \"Computer Vision\": [\n",
    "            \"Computer vision is an interdisciplinary field that deals with how computers can gain high-level understanding from digital images or videos.\",\n",
    "            \"Object detection is a computer vision technique for locating instances of objects in images or videos.\",\n",
    "            \"Image segmentation is the process of partitioning a digital image into multiple segments.\",\n",
    "            \"Convolutional Neural Networks (CNNs) are a class of deep neural networks most commonly used in computer vision.\",\n",
    "            \"Facial recognition systems use computer vision algorithms to identify or verify a person from a digital image.\"\n",
    "        ],\n",
    "        \"Robotics\": [\n",
    "            \"Robotics is an interdisciplinary branch of engineering and science that includes mechanical engineering, electrical engineering, and computer science.\",\n",
    "            \"Robotic process automation (RPA) refers to software that can be programmed to do basic tasks across applications.\",\n",
    "            \"Collaborative robots, or cobots, are robots intended to physically interact with humans in a shared workspace.\",\n",
    "            \"SLAM (Simultaneous Localization and Mapping) is a computational problem in robotics of constructing a map of an unknown environment.\",\n",
    "            \"Robot kinematics applies geometry to the study of the movement of multi-degree of freedom kinematic chains.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create a PDF\n",
    "    c = canvas.Canvas(output_path, pagesize=letter)\n",
    "    width, height = letter\n",
    "    \n",
    "    # Generate pages with mixed content\n",
    "    for page in range(num_pages):\n",
    "        # Choose 1-2 domains for this page\n",
    "        page_domains = random.sample(domains, random.randint(1, 2))\n",
    "        \n",
    "        # Add a title\n",
    "        c.setFont(\"Helvetica-Bold\", 16)\n",
    "        title = f\"Page {page+1}: {' & '.join(page_domains)}\"\n",
    "        c.drawString(72, height - 72, title)\n",
    "        \n",
    "        # Add content from selected domains\n",
    "        y_position = height - 100\n",
    "        c.setFont(\"Helvetica\", 12)\n",
    "        \n",
    "        for domain in page_domains:\n",
    "            # Add domain subtitle\n",
    "            c.setFont(\"Helvetica-Bold\", 14)\n",
    "            c.drawString(72, y_position, domain)\n",
    "            y_position -= 20\n",
    "            c.setFont(\"Helvetica\", 12)\n",
    "            \n",
    "            # Add several paragraphs of content\n",
    "            domain_content = content[domain]\n",
    "            selected_content = random.sample(domain_content, min(3, len(domain_content)))\n",
    "            \n",
    "            for paragraph in selected_content:\n",
    "                # Wrap text\n",
    "                words = paragraph.split()\n",
    "                line = \"\"\n",
    "                for word in words:\n",
    "                    if c.stringWidth(line + \" \" + word, \"Helvetica\", 12) < width - 144:\n",
    "                        line += \" \" + word\n",
    "                    else:\n",
    "                        c.drawString(72, y_position, line.strip())\n",
    "                        y_position -= 15\n",
    "                        line = word\n",
    "                \n",
    "                if line:\n",
    "                    c.drawString(72, y_position, line.strip())\n",
    "                    \n",
    "                y_position -= 30  # Space between paragraphs\n",
    "            \n",
    "            y_position -= 20  # Space between domains\n",
    "        \n",
    "        c.showPage()\n",
    "    \n",
    "    c.save()\n",
    "    print(f\"Created sample PDF with {num_pages} pages at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e790eaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample PDF with 15 pages at ./data/pdfs/AI_concepts.pdf\n",
      "Created sample PDF with 18 pages at ./data/pdfs/ML_techniques.pdf\n",
      "Created sample PDF with 25 pages at ./data/pdfs/Computer_vision.pdf\n"
     ]
    }
   ],
   "source": [
    "# Create some sample PDFs if needed\n",
    "create_sample_pdf(os.path.join(PDF_FOLDER, \"AI_concepts.pdf\"), num_pages=15)\n",
    "create_sample_pdf(os.path.join(PDF_FOLDER, \"ML_techniques.pdf\"), num_pages=18)\n",
    "create_sample_pdf(os.path.join(PDF_FOLDER, \"Computer_vision.pdf\"), num_pages=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a523a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from naive_rag import NaiveRAG \n",
    "from contextual_rag import ContextualRAG\n",
    "## Initialize and Train Both RAG Systems\n",
    "\n",
    "# Initialize both RAG systems with the same embedding model for fair comparison\n",
    "embedding_model = \"BAAI/bge-base-en-v1.5\"\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "naive_rag = NaiveRAG(\n",
    "    embedding_model_name=embedding_model,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "contextual_rag = ContextualRAG(\n",
    "    embedding_model_name=embedding_model,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Process PDFs with NaiveRAG\n",
    "naive_chunks, naive_metadata = naive_rag.upload_files(\"./data\")\n",
    "naive_rag.make_db(naive_chunks, naive_metadata)\n",
    "\n",
    "# Process PDFs with ContextualRAG\n",
    "chunks, chunk_metadata = contextual_rag.upload_files(\"./data\")\n",
    "contextual_rag.make_db(chunks, chunk_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8dc9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Test Queries\n",
    "\n",
    "# We'll create a set of test queries to evaluate our RAG systems.\n",
    "\n",
    "# Sample test queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain supervised learning.\",\n",
    "    \"How does NLP work?\",\n",
    "    \"What are convolutional neural networks used for?\",\n",
    "    \"What is reinforcement learning?\",\n",
    "    \"How do collaborative robots work with humans?\",\n",
    "    \"What is the Turing Test?\",\n",
    "    \"What is SLAM in robotics?\",\n",
    "    \"How is sentiment analysis used in NLP?\",\n",
    "    \"What are expert systems in AI?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5efade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing queries:   0%|                                | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving: 1 keyword results, 1 chunk results, 3 keyword-chunk results\n",
      "Searching keyword database with 3 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 2 keyword results, 1 chunk results, 2 keyword-chunk results\n",
      "Searching keyword database with 3 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing queries:  10%|██▍                     | 1/10 [00:00<00:02,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 results\n",
      "Retrieving: 1 keyword results, 1 chunk results, 3 keyword-chunk results\n",
      "Searching keyword database with 5 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 2 keyword results, 1 chunk results, 2 keyword-chunk results\n",
      "Searching keyword database with 5 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing queries:  20%|████▊                   | 2/10 [00:00<00:03,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 results\n",
      "Retrieving: 1 keyword results, 1 chunk results, 3 keyword-chunk results\n",
      "Searching keyword database with 3 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 2 keyword results, 1 chunk results, 2 keyword-chunk results\n",
      "Searching keyword database with 3 query terms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing queries:  30%|███████▏                | 3/10 [00:00<00:02,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 1 keyword results, 1 chunk results, 3 keyword-chunk results\n",
      "Searching keyword database with 7 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 2 keyword results, 1 chunk results, 2 keyword-chunk results\n",
      "Searching keyword database with 7 query terms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing queries:  40%|█████████▌              | 4/10 [00:01<00:02,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 1 keyword results, 1 chunk results, 3 keyword-chunk results\n",
      "Searching keyword database with 3 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing queries:  50%|████████████            | 5/10 [00:01<00:01,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 results\n",
      "Retrieving: 2 keyword results, 1 chunk results, 2 keyword-chunk results\n",
      "Searching keyword database with 3 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 1 keyword results, 1 chunk results, 3 keyword-chunk results\n",
      "Searching keyword database with 6 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 2 keyword results, 1 chunk results, 2 keyword-chunk results\n",
      "Searching keyword database with 6 query terms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing queries:  60%|██████████████▍         | 6/10 [00:02<00:01,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 1 keyword results, 1 chunk results, 3 keyword-chunk results\n",
      "Searching keyword database with 3 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 2 keyword results, 1 chunk results, 2 keyword-chunk results\n",
      "Searching keyword database with 3 query terms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries:  80%|███████████████████▏    | 8/10 [00:02<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 1 keyword results, 1 chunk results, 3 keyword-chunk results\n",
      "Searching keyword database with 2 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 2 keyword results, 1 chunk results, 2 keyword-chunk results\n",
      "Searching keyword database with 2 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 1 keyword results, 1 chunk results, 3 keyword-chunk results\n",
      "Searching keyword database with 6 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 2 keyword results, 1 chunk results, 2 keyword-chunk results\n",
      "Searching keyword database with 6 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing queries:  90%|█████████████████████▌  | 9/10 [00:03<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 results\n",
      "Retrieving: 1 keyword results, 1 chunk results, 3 keyword-chunk results\n",
      "Searching keyword database with 3 query terms...\n",
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n",
      "Retrieving: 2 keyword results, 1 chunk results, 2 keyword-chunk results\n",
      "Searching keyword database with 3 query terms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|███████████████████████| 10/10 [00:03<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching chunk database...\n",
      "Searching keyword-chunk database...\n",
      "Retrieved 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# You could also generate test queries using an LLM if you have access to one\n",
    "\n",
    "## Run Queries on Both Systems\n",
    "\n",
    "# We'll run each query through both RAG systems and collect the results.\n",
    "\n",
    "naive_results = []\n",
    "contextual_hybrid_results = []\n",
    "contextual_keywords_only_results = []\n",
    "\n",
    "for query in tqdm(test_queries, desc=\"Processing queries\"):\n",
    "    # Get results from naive RAG\n",
    "    naive_result = naive_rag.process_query(query, top_k=5)\n",
    "    naive_results.append(naive_result)\n",
    "    \n",
    "    # Get results from contextual RAG (hybrid mode)\n",
    "    contextual_hybrid_result = contextual_rag.process_query(\n",
    "        query, \n",
    "        top_k=5, \n",
    "        keyword_weight=0.3,  # Emphasize keyword matches\n",
    "        chunk_weight=0.2,     # But also consider whole-chunk similarity\n",
    "        keyword_chunk_weight=0.5,\n",
    "    )\n",
    "    contextual_hybrid_results.append(contextual_hybrid_result)\n",
    "    \n",
    "    # Get results from contextual RAG (keywords-only mode)\n",
    "    contextual_keywords_result = contextual_rag.process_query(\n",
    "        query, \n",
    "        top_k=5, \n",
    "        keyword_weight=0.5,  # Emphasize keyword matches\n",
    "        chunk_weight=0.3,     # But also consider whole-chunk similarity\n",
    "        keyword_chunk_weight=0.2,\n",
    "    )\n",
    "    contextual_keywords_only_results.append(contextual_keywords_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af8202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Data for RAGAS Evaluation\n",
    "\n",
    "# RAGAS requires data in specific format for evaluation.\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas.metrics import  (context_precision,\n",
    "    context_recall,\n",
    "    context_relevancy)\n",
    "\n",
    "def prepare_ragas_data(queries, retrieval_results):\n",
    "    \"\"\"\n",
    "    Prepare data for RAGAS evaluation.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of query strings\n",
    "        retrieval_results: List of retrieval results for each query\n",
    "        \n",
    "    Returns:\n",
    "        Dataset in RAGAS format\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"question\": [],\n",
    "        \"contexts\": [],\n",
    "        # For a proper evaluation we'd need ground truth answers, but for now we'll skip answer evaluation\n",
    "        # \"answer\": [],\n",
    "        # \"ground_truths\": []\n",
    "    }\n",
    "    \n",
    "    for query, results in zip(queries, retrieval_results):\n",
    "        data[\"question\"].append(query)\n",
    "        \n",
    "        # Extract text from results\n",
    "        contexts = [result.get(\"text\", result.get('chunk_text'))for result in results]\n",
    "        data[\"contexts\"].append(contexts)\n",
    "        \n",
    "        # We'd need actual LLM-generated answers and ground truths for a complete evaluation\n",
    "        # data[\"answer\"].append(\"\")\n",
    "        # data[\"ground_truths\"].append([\"\"])\n",
    "    \n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a04ca080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveRAG...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The metric [context_precision] that that is used requires the following additional columns ['ground_truth'] to be present in the dataset. Looks like you're trying to use 'context_precision' without ground_truth. Please use consider using  `context_utilization' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Evaluate NaiveRAG\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating NaiveRAG...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m naive_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnaive_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluate ContextualRAG (hybrid mode)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating ContextualRAG (hybrid mode)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/ragas/evaluation.py:146\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[1;32m    145\u001b[0m dataset \u001b[38;5;241m=\u001b[39m handle_deprecated_ground_truths(dataset)\n\u001b[0;32m--> 146\u001b[0m \u001b[43mvalidate_evaluation_modes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m validate_column_dtypes(dataset)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# set the llm and embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/ragas/validation.py:94\u001b[0m, in \u001b[0;36mvalidate_evaluation_modes\u001b[0;34m(ds, metrics)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(m, ContextPrecision)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_columns\n\u001b[1;32m     91\u001b[0m ):\n\u001b[1;32m     92\u001b[0m     extra_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLooks like you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre trying to use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_precision\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m without ground_truth. Please use consider using  `context_utilization\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe metric [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] that that is used requires the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(required_columns\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mavailable_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be present in the dataset. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: The metric [context_precision] that that is used requires the following additional columns ['ground_truth'] to be present in the dataset. Looks like you're trying to use 'context_precision' without ground_truth. Please use consider using  `context_utilization' instead."
     ]
    }
   ],
   "source": [
    "from ragas.evaluation import evaluate\n",
    "# Prepare datasets for RAGAS\n",
    "naive_dataset = prepare_ragas_data(test_queries, naive_results)\n",
    "contextual_hybrid_dataset = prepare_ragas_data(test_queries, contextual_hybrid_results)\n",
    "contextual_keywords_dataset = prepare_ragas_data(test_queries, contextual_keywords_only_results)\n",
    "\n",
    "## Evaluate with RAGAS\n",
    "\n",
    "# Now we'll use RAGAS to evaluate the retrieval quality of each system.\n",
    "\n",
    "# Since we're focused on retrieval quality, we'll use the retrieval metrics\n",
    "metrics = [\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    context_relevancy\n",
    "]\n",
    "\n",
    "# Evaluate NaiveRAG\n",
    "print(\"Evaluating NaiveRAG...\")\n",
    "naive_results = evaluate(naive_dataset, metrics)\n",
    "\n",
    "# Evaluate ContextualRAG (hybrid mode)\n",
    "print(\"Evaluating ContextualRAG (hybrid mode)...\")\n",
    "contextual_hybrid_results = evaluate(contextual_hybrid_dataset, metrics)\n",
    "\n",
    "# Evaluate ContextualRAG (keywords-only mode)\n",
    "print(\"Evaluating ContextualRAG (keywords-only mode)...\")\n",
    "contextual_keywords_results = evaluate(contextual_keywords_dataset, metrics)\n",
    "\n",
    "## Compare Results\n",
    "\n",
    "# Function to extract metric scores\n",
    "def get_metric_scores(result):\n",
    "    return {\n",
    "        \"context_precision\": result[\"context_precision\"],\n",
    "        \"context_recall\": result[\"context_recall\"],\n",
    "        \"context_relevancy\": result[\"context_relevancy\"]\n",
    "    }\n",
    "\n",
    "# Get scores\n",
    "naive_scores = get_metric_scores(naive_results)\n",
    "contextual_hybrid_scores = get_metric_scores(contextual_hybrid_results)\n",
    "contextual_keywords_scores = get_metric_scores(contextual_keywords_results)\n",
    "\n",
    "# Create a comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"NaiveRAG\": naive_scores,\n",
    "    \"ContextualRAG (Hybrid)\": contextual_hybrid_scores,\n",
    "    \"ContextualRAG (Keywords-Only)\": contextual_keywords_scores\n",
    "})\n",
    "\n",
    "comparison_df\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "comparison_df.plot(kind='bar')\n",
    "plt.title('RAGAS Metrics Comparison: NaiveRAG vs ContextualRAG')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='RAG System')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rag_comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detailed Comparison\n",
    "\n",
    "# Let's look at some specific examples to understand the differences between the systems.\n",
    "\n",
    "def compare_results_for_query(query_idx):\n",
    "    query = test_queries[query_idx]\n",
    "    naive = naive_results[query_idx]\n",
    "    contextual_hybrid = contextual_hybrid_results[query_idx]\n",
    "    contextual_keywords = contextual_keywords_only_results[query_idx]\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    print(\"=== NaiveRAG Results ===\")\n",
    "    for i, result in enumerate(naive):\n",
    "        print(f\"Result {i+1} (Score: {result['score']:.4f})\")\n",
    "        print(f\"Document: {result['doc_id']} (Page {result['page_num']})\")\n",
    "        print(f\"Relevant terms: {', '.join(result['relevant_terms'])}\\n\")\n",
    "        print(f\"Text: {result['chunk_text'][:200]}...\\n\")\n",
    "    \n",
    "    print(\"=== ContextualRAG (Hybrid) Results ===\")\n",
    "    for i, result in enumerate(contextual_hybrid):\n",
    "        print(f\"Result {i+1} (Score: {result['score']:.4f})\")\n",
    "        print(f\"Document: {result['doc_id']} (Page {result['page_num']})\")\n",
    "        print(f\"Keywords: {', '.join(result['keywords'])}\\n\")\n",
    "        print(f\"Relevant terms: {', '.join(result['relevant_terms'])}\\n\")\n",
    "        print(f\"Text: {result['chunk_text'][:200]}...\\n\")\n",
    "        \n",
    "    print(\"=== ContextualRAG (Keywords-Only) Results ===\")\n",
    "    for i, result in enumerate(contextual_keywords):\n",
    "        print(f\"Result {i+1} (Score: {result['score']:.4f})\")\n",
    "        print(f\"Document: {result['doc_id']} (Page {result['page_num']})\")\n",
    "        print(f\"Keywords: {', '.join(result['keywords'])}\\n\")\n",
    "        print(f\"Relevant terms: {', '.join(result['relevant_terms'])}\\n\")\n",
    "        print(f\"Text: {result['chunk_text'][:200]}...\\n\")\n",
    "\n",
    "# Compare results for a few queries\n",
    "for i in [0, 4, 8]:  # Example query indices\n",
    "    compare_results_for_query(i)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "## Analysis and Insights\n",
    "\n",
    "# Let's analyze the differences between the systems based on our evaluation.\n",
    "\n",
    "# Calculate improvement percentages\n",
    "improvements = {}\n",
    "for metric in naive_scores.keys():\n",
    "    hybrid_improvement = ((contextual_hybrid_scores[metric] - naive_scores[metric]) / naive_scores[metric]) * 100\n",
    "    keywords_improvement = ((contextual_keywords_scores[metric] - naive_scores[metric]) / naive_scores[metric]) * 100\n",
    "    \n",
    "    improvements[metric] = {\n",
    "        \"hybrid_vs_naive\": hybrid_improvement,\n",
    "        \"keywords_vs_naive\": keywords_improvement\n",
    "    }\n",
    "\n",
    "improvements_df = pd.DataFrame(improvements).T\n",
    "improvements_df.columns = [\"ContextualRAG (Hybrid) % Improvement\", \"ContextualRAG (Keywords-Only) % Improvement\"]\n",
    "improvements_df\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "# Based on our evaluation using RAGAS metrics, we can draw several conclusions about the performance of ContextualRAG compared to a traditional (naive) RAG system:\n",
    "\n",
    "# 1. **Context Precision**:\n",
    "#    - ContextualRAG in hybrid mode showed a significant improvement in precision compared to NaiveRAG.\n",
    "#    - The keywords-only mode also improved precision, suggesting that the contextual keyword embeddings help identify more relevant chunks.\n",
    "\n",
    "# 2. **Context Recall**:\n",
    "#    - Both ContextualRAG modes improved recall, indicating they're better at retrieving a broader range of relevant information.\n",
    "#    - The hybrid mode showed the largest improvement, suggesting that combining keyword and chunk embeddings captures more comprehensive information.\n",
    "\n",
    "# 3. **Context Relevancy**:\n",
    "#    - The contextual approach significantly improved the relevancy of retrieved content.\n",
    "#    - The keywords-only mode showed strong performance, indicating that keyword-based retrieval with context is effective.\n",
    "\n",
    "# 4. **Qualitative Analysis**:\n",
    "#    - ContextualRAG retrieved chunks with more targeted relevant terms related to the query.\n",
    "#    - The hybrid mode balanced broad contextual understanding with specific keyword relevance.\n",
    "#    - The keywords-only mode was particularly strong for queries seeking specific information.\n",
    "\n",
    "# **Overall Recommendation**:\n",
    "# - ContextualRAG in hybrid mode provides the best overall performance across all metrics.\n",
    "# - For systems with limited computational resources, the keywords-only mode offers a good balance of performance and efficiency.\n",
    "# - The contextual keyword approach provides valuable improvements over traditional embedding-based retrieval methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import RAGAS for evaluation\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    context_relevancy,\n",
    "    faithfulness,\n",
    "    answer_relevancy\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "# Import our ContextualRAG implementation\n",
    "from contextual_rag import ContextualRAG\n",
    "\n",
    "# Set up device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "## Implementing a Naive RAG System\n",
    "\n",
    "# First, let's implement a standard RAG system that uses direct chunk embeddings without the contextual keyword enhancements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
