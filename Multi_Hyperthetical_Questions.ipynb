{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2588af5e",
   "metadata": {},
   "source": [
    "### Ragas is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e13d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/oleg/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/oleg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "\n",
    "from utils import OPENAI_API_KEY\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY \n",
    "#from agentic_chunker import AgenticChunker\n",
    "from llm_utils import *\n",
    "from llm_utils import _text\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "#openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2116d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6452a064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1135, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 35\n"
     ]
    }
   ],
   "source": [
    "#loader_url =SeleniumURLLoader([\"https://arxiv.org/pdf/2312.10997\",\n",
    "pdf_list = [\"./data/data_rag/s41524-023-01062-z.pdf\",\n",
    "    \"./data/data_rag/s41699-023-00369-1.pdf\"\n",
    "         #   \"https://github.com/HSE-LAMBDA/ai4material_design/tree/main/docs/CONSTRUCTOR-MOCK.md\"\n",
    "         #   \"https://github.com/HSE-LAMBDA/ai4material_design/blob/main/docs/CONSTRUCTOR.md\",\n",
    "         #   \"https://github.com/HSE-LAMBDA/ai4material_design/blob/main/docs/DATA.md\",\n",
    "         #   \"https://github.com/HSE-LAMBDA/ai4material_design/blob/main/docs/ENVIRONMENT.md\",\n",
    "         #   \"https://github.com/HSE-LAMBDA/ai4material_design/blob/main/docs/GENERATING-CONSTRUCTOR.md\",\n",
    "         #   \"https://github.com/HSE-LAMBDA/ai4material_design/blob/main/docs/GENERATING-MOCK.md\",\n",
    "         #   \"https://github.com/HSE-LAMBDA/ai4material_design/blob/main/docs/PILOT.md\",\n",
    "         #   \"https://github.com/HSE-LAMBDA/ai4material_design/blob/main/docs/SPARSE-PAPER.md\"\n",
    "          #  \"https://www.nature.com/articles/s41377-024-01407-3\",\n",
    "          #  \"https://www.nature.com/articles/s41565-023-01407-1\",\n",
    "          #  \"https://www.nature.com/articles/s41699-023-00369-1\",\n",
    "           ]\n",
    "                               \n",
    "chunks, documents = load_pdf_documets(pdf_list, tokens=1000)\n",
    "print(len(chunks), len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1277e",
   "metadata": {},
   "source": [
    "### Load test QA  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43059158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ground_true = pd.read_csv(\"./data/data_rag/QA_ai4mat_2articles.csv\")\n",
    "\n",
    "questions = df_ground_true['question'].values.tolist()\n",
    "answers = df_ground_true['answer'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9171f36",
   "metadata": {},
   "source": [
    "### Embeedding of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c82c0d",
   "metadata": {},
   "source": [
    "### Compare 2 stratigies \n",
    "#### Recursive chunking,  Multi Hypothetical Queations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db0908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    context_utilization,\n",
    "    context_entity_recall,\n",
    "    context_relevancy,\n",
    "    answer_relevancy,\n",
    "    answer_correctness, \n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "from datasets import Dataset \n",
    "\n",
    "from ragas.evaluation import evaluate\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce2d9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "   answer_relevancy,\n",
    "    answer_correctness, \n",
    "   faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    " #   context_entity_recall,\n",
    "    context_relevancy,\n",
    " #   context_utilization,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb265fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb, q_list, ch_list, dataset, result = dict(), dict(), dict(), dict(), dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af37ee2",
   "metadata": {},
   "source": [
    "### Make vectores DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b2f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(\"./milvus_demo.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a9a7d",
   "metadata": {},
   "source": [
    "#### Recursive chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53fff729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08630fde6994ba2a0f3a89faab305c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "The same chunks:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(128, 1536, True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'recursive'\n",
    "emb[name], q_list[name], ch_list[name] = get_embs(chunks, embedding=OpenAIEmbeddings(),  question='no') \n",
    "\n",
    "len(emb[name]), len(emb[name][0]), len(ch_list[name])==len(emb[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a52ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension 1536\n"
     ]
    }
   ],
   "source": [
    "collection_name = f\"ai4mat_OpenAiEmb_{name}\"\n",
    "\n",
    "if milvus_client.has_collection(collection_name):\n",
    "    milvus_client.drop_collection(collection_name)\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=len(emb[name][0]),    # emb dimension\n",
    "    metric_type=\"IP\",            # Inner product distance\n",
    "    consistency_level=\"Strong\",  # Strong consistency level\n",
    "    )\n",
    "print(\"dimension\",len(emb[name][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d42fb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|█████████████████████████| 128/128 [00:00<00:00, 164734.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'insert_count': 128,\n",
       " 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127],\n",
       " 'cost': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i, line in enumerate(tqdm(ch_list[name], desc=\"Creating embeddings\")):\n",
    "    data.append({\"id\": i, \"vector\": emb[name][i], \"text\": _text(ch_list[name][i]), })\n",
    "\n",
    "\n",
    "\n",
    "milvus_client.insert(collection_name=collection_name, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1c68d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dd3b6cd2db4cfbbb83f0f38a9e3e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[name] =  get_datasamples(questions, \n",
    "                            answers, \n",
    "                            milvus_client, \n",
    "                            collection_name, \n",
    "                            embedding=lambda q: OpenAIEmbeddings().embed_query(q), \n",
    "                            llm = llm,\n",
    "                            top_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032a4950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90240bddfe243968cd1acb120336e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "Failed to parse output. Returning None.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "Failed to parse output. Returning None.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "#query_engine_bge = build_query_engine(hf_bge, documents)\n",
    "result[name] = evaluate(dataset[name], metrics, llm=llm,  raise_exceptions=False, embeddings=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4b7c1",
   "metadata": {},
   "source": [
    "#### Multi Hypothetical Queations for Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c32c63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f32b10e62444b0a6ddbf115a2f3d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating questions & split:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(281, 1536, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'questions'\n",
    "emb[name], q_list[name], ch_list[name] = get_embs(documents, embedding=OpenAIEmbeddings(),  question='split') \n",
    "\n",
    "len(emb[name]), len(emb[name][0]), len(ch_list[name])==len(emb[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1159c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension 1536\n"
     ]
    }
   ],
   "source": [
    "collection_name = f\"ai4mat_OpenAiEmb_{name}\"\n",
    "\n",
    "if milvus_client.has_collection(collection_name):\n",
    "    milvus_client.drop_collection(collection_name)\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=len(emb[name][0]),    # emb dimension\n",
    "    metric_type=\"IP\",            # Inner product distance\n",
    "    consistency_level=\"Strong\",  # Strong consistency level\n",
    "    )\n",
    "print(\"dimension\",len(emb[name][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f8bfb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|█████████████████████████| 281/281 [00:00<00:00, 176199.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'insert_count': 281,\n",
       " 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280],\n",
       " 'cost': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i, line in enumerate(tqdm(ch_list[name], desc=\"Creating embeddings\")):\n",
    "    data.append({\"id\": i, \"vector\": emb[name][i], \"text\": _text(ch_list[name][i]), })\n",
    "\n",
    "milvus_client.insert(collection_name=collection_name, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d74bd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf9cab49fe544c28b4f5b77342ed8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[name] =  get_datasamples(questions, \n",
    "                            answers, \n",
    "                            milvus_client, \n",
    "                            collection_name, \n",
    "                            embedding=lambda q: OpenAIEmbeddings().embed_query(q), \n",
    "                            llm = llm,\n",
    "                            top_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a4f9a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66887c7496124783bdf0a5c3e36b6187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "#query_engine_bge = build_query_engine(hf_bge, documents)\n",
    "result[name] = evaluate(dataset[name], metrics, llm=llm,  raise_exceptions=False, embeddings=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900a3986",
   "metadata": {},
   "source": [
    "#### Multi Hypothetical Queations for Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1b17a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1777a04341304a80bc1ec9cec67fb560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating questions:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(930, 1536, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'ch_questions'\n",
    "emb[name], q_list[name], ch_list[name] = get_embs(chunks, embedding=OpenAIEmbeddings(),  question='chunk') \n",
    "\n",
    "len(emb[name]), len(emb[name][0]), len(ch_list[name])==len(emb[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5828118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension 1536\n"
     ]
    }
   ],
   "source": [
    "collection_name = f\"ai4mat_OpenAiEmb_{name}\"\n",
    "\n",
    "if milvus_client.has_collection(collection_name):\n",
    "    milvus_client.drop_collection(collection_name)\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=len(emb[name][0]),    # emb dimension\n",
    "    metric_type=\"IP\",            # Inner product distance\n",
    "    consistency_level=\"Strong\",  # Strong consistency level\n",
    "    )\n",
    "print(\"dimension\",len(emb[name][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69ea2bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████████████████████| 930/930 [00:00<00:00, 32260.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'insert_count': 930,\n",
       " 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929],\n",
       " 'cost': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i, line in enumerate(tqdm(ch_list[name], desc=\"Creating embeddings\")):\n",
    "    data.append({\"id\": i, \"vector\": emb[name][i], \"text\": _text(ch_list[name][i]), })\n",
    "\n",
    "milvus_client.insert(collection_name=collection_name, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17250fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1410d56f791f463da39245846529fdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[name] =  get_datasamples(questions, \n",
    "                            answers, \n",
    "                            milvus_client, \n",
    "                            collection_name, \n",
    "                            embedding=lambda q: OpenAIEmbeddings().embed_query(q), \n",
    "                            llm = llm,\n",
    "                            top_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49cd82a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd696689ae941fb93dd46b4f4f5fbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "#query_engine_bge = build_query_engine(hf_bge, documents)\n",
    "result[name] = evaluate(dataset[name], metrics, llm=llm,  raise_exceptions=False, embeddings=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b474821",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e33513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(d, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints a dictionary, including nested dictionaries, \n",
    "    with indentation to represent structure.\n",
    "    \"\"\"\n",
    "    for key, value in d.items():\n",
    "        print('  ' * indent + str(key) + \": \", end=\"\")\n",
    "        if isinstance(value, dict):\n",
    "            print()  # Print a newline for nested dictionaries\n",
    "            print_dict(value, indent + 1)\n",
    "        else:\n",
    "            print(str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42f7bebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recursive: \n",
      "  answer_relevancy: 0.2251211414806174\n",
      "  answer_correctness: 0.25021229760513114\n",
      "  faithfulness: 1.0\n",
      "  context_precision: 0.476958931701444\n",
      "  context_recall: 0.5095238095238095\n",
      "  context_relevancy: 0.012575242305061787\n",
      "questions: \n",
      "  answer_relevancy: 0.25052256382601473\n",
      "  answer_correctness: 0.29252062765876996\n",
      "  faithfulness: 0.9875\n",
      "  context_precision: 0.4810856143822227\n",
      "  context_recall: 0.44682539682539685\n",
      "  context_relevancy: 0.08773145212595819\n",
      "ch_questions: \n",
      "  answer_relevancy: 0.22492653933293516\n",
      "  answer_correctness: 0.28768316701284585\n",
      "  faithfulness: 1.0\n",
      "  context_precision: 0.5184657659191172\n",
      "  context_recall: 0.43697478991596633\n",
      "  context_relevancy: 0.012016894449205463\n"
     ]
    }
   ],
   "source": [
    "print_dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f189caf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recursive</th>\n",
       "      <td>0.225121</td>\n",
       "      <td>0.250212</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.476959</td>\n",
       "      <td>0.509524</td>\n",
       "      <td>0.012575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>questions</th>\n",
       "      <td>0.250523</td>\n",
       "      <td>0.292521</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.481086</td>\n",
       "      <td>0.446825</td>\n",
       "      <td>0.087731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch_questions</th>\n",
       "      <td>0.224927</td>\n",
       "      <td>0.287683</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.518466</td>\n",
       "      <td>0.436975</td>\n",
       "      <td>0.012017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              answer_relevancy  answer_correctness  faithfulness  \\\n",
       "recursive             0.225121            0.250212        1.0000   \n",
       "questions             0.250523            0.292521        0.9875   \n",
       "ch_questions          0.224927            0.287683        1.0000   \n",
       "\n",
       "              context_precision  context_recall  context_relevancy  \n",
       "recursive              0.476959        0.509524           0.012575  \n",
       "questions              0.481086        0.446825           0.087731  \n",
       "ch_questions           0.518466        0.436975           0.012017  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m = pd.DataFrame().from_dict(result, orient='index')\n",
    "\n",
    "df_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.to_csv('df_compare_multi_questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae865cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset['recursive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset['ch_questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7309d79e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
